{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7de0ab",
   "metadata": {},
   "source": [
    "# Advanced Pandas Techniques\n",
    "\n",
    "This notebook covers advanced Pandas features and techniques for more complex data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39b16ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas imported successfully!\n",
      "Pandas version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Pandas imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785b2d0",
   "metadata": {},
   "source": [
    "## Advanced Indexing with MultiIndex\n",
    "\n",
    "MultiIndex allows you to have multiple levels of indexing, enabling hierarchical data structures for complex data organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e44170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with MultiIndex:\n",
      "                     X         Y\n",
      "first second                    \n",
      "A     one     0.875756 -0.249367\n",
      "      two    -0.251523  1.227751\n",
      "B     one     0.286285  1.903437\n",
      "      two     1.302904  0.579303\n",
      "\n",
      "DataFrame with MultiIndex from tuples:\n",
      "                     X         Y\n",
      "first second                    \n",
      "A     one     1.256785 -1.143781\n",
      "      two     0.192862  0.146536\n",
      "B     one    -1.003739 -0.650799\n",
      "      two     0.768055 -0.275589\n",
      "\n",
      "Accessing level 0 'A':\n",
      "               X         Y\n",
      "second                    \n",
      "one     0.875756 -0.249367\n",
      "two    -0.251523  1.227751\n",
      "\n",
      "Accessing specific combination ('A', 'one'):\n",
      "X    0.875756\n",
      "Y   -0.249367\n",
      "Name: (A, one), dtype: float64\n",
      "\n",
      "Cross-section for second level 'one':\n",
      "              X         Y\n",
      "first                    \n",
      "A      0.875756 -0.249367\n",
      "B      0.286285  1.903437\n",
      "\n",
      "DataFrame with MultiIndex columns:\n",
      "     Math       Science      \n",
      "  Midterm Final Midterm Final\n",
      "0      70    58      72    56\n",
      "1      72    96      94    64\n",
      "2      97    81      68    64\n",
      "\n",
      "Accessing Math scores:\n",
      "   Midterm  Final\n",
      "0       70     58\n",
      "1       72     96\n",
      "2       97     81\n",
      "\n",
      "Accessing Midterm scores for all subjects:\n",
      "   Math  Science\n",
      "0    70       72\n",
      "1    72       94\n",
      "2    97       68\n",
      "\n",
      "Stacked DataFrame:\n",
      "first  second   \n",
      "A      one     X    0.875756\n",
      "               Y   -0.249367\n",
      "       two     X   -0.251523\n",
      "               Y    1.227751\n",
      "B      one     X    0.286285\n",
      "               Y    1.903437\n",
      "       two     X    1.302904\n",
      "               Y    0.579303\n",
      "dtype: float64\n",
      "\n",
      "Unstacked DataFrame:\n",
      "                     X         Y\n",
      "first second                    \n",
      "A     one     0.875756 -0.249367\n",
      "      two    -0.251523  1.227751\n",
      "B     one     0.286285  1.903437\n",
      "      two     1.302904  0.579303\n",
      "\n",
      "Grouped by MultiIndex levels:\n",
      "                     X         Y\n",
      "first second                    \n",
      "A     one     0.875756 -0.249367\n",
      "      two    -0.251523  1.227751\n",
      "B     one     0.286285  1.903437\n",
      "      two     1.302904  0.579303\n"
     ]
    }
   ],
   "source": [
    "# MultiIndex Examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create MultiIndex from arrays\n",
    "arrays = [['A', 'A', 'B', 'B'], ['one', 'two', 'one', 'two']]\n",
    "index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])\n",
    "df = pd.DataFrame(np.random.randn(4, 2), index=index, columns=['X', 'Y'])\n",
    "print(\"DataFrame with MultiIndex:\")\n",
    "print(df)\n",
    "\n",
    "# Create MultiIndex from tuples\n",
    "tuples = [('A', 'one'), ('A', 'two'), ('B', 'one'), ('B', 'two')]\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n",
    "df_tuples = pd.DataFrame(np.random.randn(4, 2), index=index, columns=['X', 'Y'])\n",
    "print(\"\\nDataFrame with MultiIndex from tuples:\")\n",
    "print(df_tuples)\n",
    "\n",
    "# Accessing data with MultiIndex\n",
    "print(\"\\nAccessing level 0 'A':\")\n",
    "print(df.loc['A'])\n",
    "\n",
    "print(\"\\nAccessing specific combination ('A', 'one'):\")\n",
    "print(df.loc[('A', 'one')])\n",
    "\n",
    "# Cross-section with xs()\n",
    "print(\"\\nCross-section for second level 'one':\")\n",
    "print(df.xs('one', level='second'))\n",
    "\n",
    "# MultiIndex for columns\n",
    "columns = pd.MultiIndex.from_arrays([['Math', 'Math', 'Science', 'Science'], ['Midterm', 'Final', 'Midterm', 'Final']])\n",
    "df_multi_col = pd.DataFrame(np.random.randint(50, 100, (3, 4)), columns=columns)\n",
    "print(\"\\nDataFrame with MultiIndex columns:\")\n",
    "print(df_multi_col)\n",
    "\n",
    "# Accessing MultiIndex columns\n",
    "print(\"\\nAccessing Math scores:\")\n",
    "print(df_multi_col['Math'])\n",
    "\n",
    "print(\"\\nAccessing Midterm scores for all subjects:\")\n",
    "print(df_multi_col.xs('Midterm', axis=1, level=1))\n",
    "\n",
    "# Stacking and unstacking\n",
    "stacked = df.stack()\n",
    "print(\"\\nStacked DataFrame:\")\n",
    "print(stacked)\n",
    "\n",
    "unstacked = stacked.unstack()\n",
    "print(\"\\nUnstacked DataFrame:\")\n",
    "print(unstacked)\n",
    "\n",
    "# Grouping with MultiIndex\n",
    "df_multi = df.reset_index()\n",
    "grouped = df_multi.groupby(['first', 'second']).mean()\n",
    "print(\"\\nGrouped by MultiIndex levels:\")\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820f1e4",
   "metadata": {},
   "source": [
    "## Working with Categorical Data\n",
    "\n",
    "Categorical data types in Pandas provide efficient storage and operations for data with a limited number of possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "341e2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name Department Performance     City\n",
      "0    Alice         HR        Good      NYC\n",
      "1      Bob         IT   Excellent       LA\n",
      "2  Charlie    Finance        Good  Chicago\n",
      "3    Diana         IT     Average      NYC\n",
      "4      Eve         HR   Excellent       LA\n",
      "\n",
      "Data types:\n",
      "Name           object\n",
      "Department     object\n",
      "Performance    object\n",
      "City           object\n",
      "dtype: object\n",
      "\n",
      "Data types after conversion:\n",
      "Name             object\n",
      "Department     category\n",
      "Performance    category\n",
      "City           category\n",
      "dtype: object\n",
      "\n",
      "Department categories: Index(['Finance', 'HR', 'IT'], dtype='object')\n",
      "Performance categories: Index(['Average', 'Excellent', 'Good'], dtype='object')\n",
      "\n",
      "Ordered Performance categories: Index(['Poor', 'Average', 'Good', 'Excellent'], dtype='object')\n",
      "Is Performance ordered: True\n",
      "\n",
      "Performance codes: 0    2\n",
      "1    3\n",
      "2    2\n",
      "3    1\n",
      "4    3\n",
      "dtype: int8\n",
      "Department value counts:\n",
      "Department\n",
      "HR         2\n",
      "IT         2\n",
      "Finance    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Department categories after adding: Index(['Finance', 'HR', 'IT', 'Marketing'], dtype='object')\n",
      "Department categories after removing: Index(['HR', 'IT', 'Marketing'], dtype='object')\n",
      "Department categories after renaming: Index(['Human Resources', 'Information Technology', 'Marketing'], dtype='object')\n",
      "\n",
      "Memory usage comparison:\n",
      "String columns: 538132 bytes\n",
      "Categorical columns: 10673 bytes\n",
      "Memory saved: 527459 bytes\n",
      "\n",
      "Grouped by Department and Performance:\n",
      "Department              Performance\n",
      "Human Resources         Good           1\n",
      "                        Excellent      1\n",
      "                        Poor           0\n",
      "                        Average        0\n",
      "Information Technology  Average        1\n",
      "                        Excellent      1\n",
      "                        Poor           0\n",
      "                        Good           0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DataFrame sorted by Performance:\n",
      "      Name Performance\n",
      "3    Diana     Average\n",
      "0    Alice        Good\n",
      "2  Charlie        Good\n",
      "1      Bob   Excellent\n",
      "4      Eve   Excellent\n"
     ]
    }
   ],
   "source": [
    "# Categorical Data Examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create DataFrame with categorical data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'],\n",
    "    'Performance': ['Good', 'Excellent', 'Good', 'Average', 'Excellent'],\n",
    "    'City': ['NYC', 'LA', 'Chicago', 'NYC', 'LA']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "\n",
    "# Convert to categorical\n",
    "df['Department'] = df['Department'].astype('category')\n",
    "df['Performance'] = df['Performance'].astype('category')\n",
    "df['City'] = df['City'].astype('category')\n",
    "\n",
    "print(f\"\\nData types after conversion:\\n{df.dtypes}\")\n",
    "print(f\"\\nDepartment categories: {df['Department'].cat.categories}\")\n",
    "print(f\"Performance categories: {df['Performance'].cat.categories}\")\n",
    "\n",
    "# Ordered categorical\n",
    "performance_order = ['Poor', 'Average', 'Good', 'Excellent']\n",
    "df['Performance'] = pd.Categorical(df['Performance'], categories=performance_order, ordered=True)\n",
    "print(f\"\\nOrdered Performance categories: {df['Performance'].cat.categories}\")\n",
    "print(f\"Is Performance ordered: {df['Performance'].cat.ordered}\")\n",
    "\n",
    "# Categorical operations\n",
    "print(f\"\\nPerformance codes: {df['Performance'].cat.codes}\")\n",
    "print(f\"Department value counts:\\n{df['Department'].value_counts()}\")\n",
    "\n",
    "# Adding new categories\n",
    "df['Department'] = df['Department'].cat.add_categories(['Marketing'])\n",
    "print(f\"\\nDepartment categories after adding: {df['Department'].cat.categories}\")\n",
    "\n",
    "# Removing categories\n",
    "df['Department'] = df['Department'].cat.remove_categories(['Finance'])\n",
    "print(f\"Department categories after removing: {df['Department'].cat.categories}\")\n",
    "\n",
    "# Renaming categories\n",
    "df['Department'] = df['Department'].cat.rename_categories({'HR': 'Human Resources', 'IT': 'Information Technology'})\n",
    "print(f\"Department categories after renaming: {df['Department'].cat.categories}\")\n",
    "\n",
    "# Memory usage comparison\n",
    "df_string = pd.DataFrame({\n",
    "    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR'] * 1000,\n",
    "    'Performance': ['Good', 'Excellent', 'Good', 'Average', 'Excellent'] * 1000\n",
    "})\n",
    "\n",
    "df_cat = df_string.copy()\n",
    "df_cat['Department'] = df_cat['Department'].astype('category')\n",
    "df_cat['Performance'] = df_cat['Performance'].astype('category')\n",
    "\n",
    "print(f\"\\nMemory usage comparison:\")\n",
    "print(f\"String columns: {df_string.memory_usage(deep=True).sum()} bytes\")\n",
    "print(f\"Categorical columns: {df_cat.memory_usage(deep=True).sum()} bytes\")\n",
    "print(f\"Memory saved: {df_string.memory_usage(deep=True).sum() - df_cat.memory_usage(deep=True).sum()} bytes\")\n",
    "\n",
    "# Grouping with categorical data\n",
    "grouped_cat = df.groupby('Department', observed=True)['Performance'].value_counts()\n",
    "print(f\"\\nGrouped by Department and Performance:\\n{grouped_cat}\")\n",
    "\n",
    "# Sorting with ordered categorical\n",
    "df_sorted = df.sort_values('Performance')\n",
    "print(f\"\\nDataFrame sorted by Performance:\\n{df_sorted[['Name', 'Performance']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bf3fa",
   "metadata": {},
   "source": [
    "## String/Text Operations\n",
    "\n",
    "Pandas provides powerful string operations through the `.str` accessor, allowing vectorized string manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7c4d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "            Name                     Email                  Description  \\\n",
      "0  Alice Johnson   alice.johnson@email.com  Data Scientist at Tech Corp   \n",
      "1      Bob Smith     bob.smith@company.org            Software Engineer   \n",
      "2  Charlie Brown  charlie.brown@school.edu        Student at University   \n",
      "3   Diana Prince     diana.prince@hero.net              Project Manager   \n",
      "4     Eve Wilson       eve.wilson@work.com         Marketing Specialist   \n",
      "\n",
      "            Phone  \n",
      "0  (555) 123-4567  \n",
      "1    555-987-6543  \n",
      "2  (555) 456-7890  \n",
      "3    555.234.5678  \n",
      "4  (555) 345-6789  \n",
      "\n",
      "Name lengths:\n",
      "0    13\n",
      "1     9\n",
      "2    13\n",
      "3    12\n",
      "4    10\n",
      "Name: Name, dtype: int64\n",
      "\n",
      "Names in uppercase:\n",
      "0    ALICE JOHNSON\n",
      "1        BOB SMITH\n",
      "2    CHARLIE BROWN\n",
      "3     DIANA PRINCE\n",
      "4       EVE WILSON\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Names in lowercase:\n",
      "0    alice johnson\n",
      "1        bob smith\n",
      "2    charlie brown\n",
      "3     diana prince\n",
      "4       eve wilson\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Names title case:\n",
      "0    Alice Johnson\n",
      "1        Bob Smith\n",
      "2    Charlie Brown\n",
      "3     Diana Prince\n",
      "4       Eve Wilson\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Split names into first and last:\n",
      "  First Name Last Name\n",
      "0      Alice   Johnson\n",
      "1        Bob     Smith\n",
      "2    Charlie     Brown\n",
      "3      Diana    Prince\n",
      "4        Eve    Wilson\n",
      "\n",
      "Extract domain from email:\n",
      "0      email.com\n",
      "1    company.org\n",
      "2     school.edu\n",
      "3       hero.net\n",
      "4       work.com\n",
      "Name: 1, dtype: object\n",
      "\n",
      "Extract area code from phone:\n",
      "     0\n",
      "0  555\n",
      "1  NaN\n",
      "2  555\n",
      "3  NaN\n",
      "4  555\n",
      "\n",
      "Check if email contains 'com':\n",
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4     True\n",
      "Name: Email, dtype: bool\n",
      "\n",
      "Replace 'com' with 'org' in emails:\n",
      "0     alice.johnson@email.org\n",
      "1       bob.smith@orgpany.org\n",
      "2    charlie.brown@school.edu\n",
      "3       diana.prince@hero.net\n",
      "4         eve.wilson@work.org\n",
      "Name: Email, dtype: object\n",
      "\n",
      "Names starting with 'A' or 'B':\n",
      "            Name                    Email                  Description  \\\n",
      "0  Alice Johnson  alice.johnson@email.com  Data Scientist at Tech Corp   \n",
      "1      Bob Smith    bob.smith@company.org            Software Engineer   \n",
      "\n",
      "            Phone  \n",
      "0  (555) 123-4567  \n",
      "1    555-987-6543  \n",
      "\n",
      "Descriptions containing 'at':\n",
      "            Name                     Email                  Description  \\\n",
      "0  Alice Johnson   alice.johnson@email.com  Data Scientist at Tech Corp   \n",
      "2  Charlie Brown  charlie.brown@school.edu        Student at University   \n",
      "\n",
      "            Phone  \n",
      "0  (555) 123-4567  \n",
      "2  (555) 456-7890  \n",
      "\n",
      "Clean phone numbers (remove non-digits):\n",
      "0    5551234567\n",
      "1    5559876543\n",
      "2    5554567890\n",
      "3    5552345678\n",
      "4    5553456789\n",
      "Name: Phone, dtype: object\n",
      "\n",
      "Pad names to length 15:\n",
      "0    Alice Johnson--\n",
      "1    Bob Smith------\n",
      "2    Charlie Brown--\n",
      "3    Diana Prince---\n",
      "4    Eve Wilson-----\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Strip whitespace:\n",
      "0     hello\n",
      "1     world\n",
      "2    pandas\n",
      "dtype: object\n",
      "\n",
      "Extract words from description:\n",
      "0    [Data, Scientist, at, Tech, Corp]\n",
      "1                 [Software, Engineer]\n",
      "2            [Student, at, University]\n",
      "3                   [Project, Manager]\n",
      "4              [Marketing, Specialist]\n",
      "Name: Description, dtype: object\n",
      "\n",
      "Count words in description:\n",
      "0    5\n",
      "1    2\n",
      "2    3\n",
      "3    2\n",
      "4    2\n",
      "Name: Description, dtype: int64\n",
      "\n",
      "DataFrame with missing text:\n",
      "            Name                     Email                  Description  \\\n",
      "0  Alice Johnson   alice.johnson@email.com  Data Scientist at Tech Corp   \n",
      "1      Bob Smith     bob.smith@company.org            Software Engineer   \n",
      "2  Charlie Brown  charlie.brown@school.edu                         None   \n",
      "3   Diana Prince     diana.prince@hero.net              Project Manager   \n",
      "4     Eve Wilson       eve.wilson@work.com         Marketing Specialist   \n",
      "\n",
      "            Phone  \n",
      "0  (555) 123-4567  \n",
      "1    555-987-6543  \n",
      "2  (555) 456-7890  \n",
      "3    555.234.5678  \n",
      "4  (555) 345-6789  \n",
      "\n",
      "Fill missing descriptions:\n",
      "0    Data Scientist at Tech Corp\n",
      "1              Software Engineer\n",
      "2                  Not specified\n",
      "3                Project Manager\n",
      "4           Marketing Specialist\n",
      "Name: Description, dtype: object\n",
      "\n",
      "Check for missing text:\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4    False\n",
      "Name: Description, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# String Operations Examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create DataFrame with text data\n",
    "data = {\n",
    "    'Name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Prince', 'Eve Wilson'],\n",
    "    'Email': ['alice.johnson@email.com', 'bob.smith@company.org', 'charlie.brown@school.edu', 'diana.prince@hero.net', 'eve.wilson@work.com'],\n",
    "    'Description': ['Data Scientist at Tech Corp', 'Software Engineer', 'Student at University', 'Project Manager', 'Marketing Specialist'],\n",
    "    'Phone': ['(555) 123-4567', '555-987-6543', '(555) 456-7890', '555.234.5678', '(555) 345-6789']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Basic string operations\n",
    "print(\"\\nName lengths:\")\n",
    "print(df['Name'].str.len())\n",
    "\n",
    "print(\"\\nNames in uppercase:\")\n",
    "print(df['Name'].str.upper())\n",
    "\n",
    "print(\"\\nNames in lowercase:\")\n",
    "print(df['Name'].str.lower())\n",
    "\n",
    "print(\"\\nNames title case:\")\n",
    "print(df['Name'].str.title())\n",
    "\n",
    "# Splitting strings\n",
    "print(\"\\nSplit names into first and last:\")\n",
    "name_split = df['Name'].str.split(' ', expand=True)\n",
    "name_split.columns = ['First Name', 'Last Name']\n",
    "print(name_split)\n",
    "\n",
    "# Extracting substrings\n",
    "print(\"\\nExtract domain from email:\")\n",
    "print(df['Email'].str.split('@', expand=True)[1])\n",
    "\n",
    "# Regular expressions\n",
    "print(\"\\nExtract area code from phone:\")\n",
    "print(df['Phone'].str.extract(r'\\((\\d{3})\\)'))\n",
    "\n",
    "print(\"\\nCheck if email contains 'com':\")\n",
    "print(df['Email'].str.contains('com'))\n",
    "\n",
    "print(\"\\nReplace 'com' with 'org' in emails:\")\n",
    "print(df['Email'].str.replace('com', 'org'))\n",
    "\n",
    "# String matching and filtering\n",
    "print(\"\\nNames starting with 'A' or 'B':\")\n",
    "print(df[df['Name'].str.startswith(('A', 'B'))])\n",
    "\n",
    "print(\"\\nDescriptions containing 'at':\")\n",
    "print(df[df['Description'].str.contains('at', case=False)])\n",
    "\n",
    "# Cleaning text data\n",
    "print(\"\\nClean phone numbers (remove non-digits):\")\n",
    "print(df['Phone'].str.replace(r'\\D', '', regex=True))\n",
    "\n",
    "# String padding\n",
    "print(\"\\nPad names to length 15:\")\n",
    "print(df['Name'].str.pad(width=15, side='right', fillchar='-'))\n",
    "\n",
    "# String stripping\n",
    "text_with_spaces = pd.Series(['  hello  ', '  world  ', ' pandas '])\n",
    "print(\"\\nStrip whitespace:\")\n",
    "print(text_with_spaces.str.strip())\n",
    "\n",
    "# Advanced operations\n",
    "print(\"\\nExtract words from description:\")\n",
    "print(df['Description'].str.findall(r'\\b\\w+\\b'))\n",
    "\n",
    "print(\"\\nCount words in description:\")\n",
    "print(df['Description'].str.split().str.len())\n",
    "\n",
    "# Working with missing text data\n",
    "df_with_na = df.copy()\n",
    "df_with_na.loc[2, 'Description'] = None\n",
    "print(\"\\nDataFrame with missing text:\")\n",
    "print(df_with_na)\n",
    "\n",
    "print(\"\\nFill missing descriptions:\")\n",
    "print(df_with_na['Description'].fillna('Not specified'))\n",
    "\n",
    "print(\"\\nCheck for missing text:\")\n",
    "print(df_with_na['Description'].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907a4ca",
   "metadata": {},
   "source": [
    "## Apply, Map, and Transform\n",
    "\n",
    "Custom functions with apply/map/transform allow you to apply complex operations to pandas data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71199297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   A   B    C Category\n",
      "0  1  10  100        A\n",
      "1  2  20  200        B\n",
      "2  3  30  300        A\n",
      "3  4  40  400        B\n",
      "4  5  50  500        A\n",
      "\n",
      "Using map() to transform Category column:\n",
      "0    Group Alpha\n",
      "1     Group Beta\n",
      "2    Group Alpha\n",
      "3     Group Beta\n",
      "4    Group Alpha\n",
      "Name: Category, dtype: object\n",
      "\n",
      "Using map() with function:\n",
      "0     1\n",
      "1     4\n",
      "2     9\n",
      "3    16\n",
      "4    25\n",
      "Name: A, dtype: int64\n",
      "\n",
      "Using apply() on Series:\n",
      "0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "4    50\n",
      "Name: A, dtype: int64\n",
      "\n",
      "Using apply() with string operations:\n",
      "0    a\n",
      "1    b\n",
      "2    a\n",
      "3    b\n",
      "4    a\n",
      "Name: Category, dtype: object\n",
      "\n",
      "Using apply() on DataFrame (axis=0, columns):\n",
      "A      15\n",
      "B     150\n",
      "C    1500\n",
      "dtype: int64\n",
      "\n",
      "Using apply() on DataFrame (axis=1, rows):\n",
      "0    111\n",
      "1    222\n",
      "2    333\n",
      "3    444\n",
      "4    555\n",
      "dtype: int64\n",
      "\n",
      "Using transform() on DataFrame:\n",
      "     A     B      C\n",
      "0 -2.0 -20.0 -200.0\n",
      "1 -1.0 -10.0 -100.0\n",
      "2  0.0   0.0    0.0\n",
      "3  1.0  10.0  100.0\n",
      "4  2.0  20.0  200.0\n",
      "\n",
      "Complex apply() example - custom function:\n",
      "0       Low\n",
      "1       Low\n",
      "2    Medium\n",
      "3    Medium\n",
      "4      High\n",
      "Name: A, dtype: object\n",
      "\n",
      "Apply with multiple columns:\n",
      "0    1-10-100\n",
      "1    2-20-200\n",
      "2    3-30-300\n",
      "3    4-40-400\n",
      "4    5-50-500\n",
      "dtype: object\n",
      "\n",
      "Transform vs Apply comparison:\n",
      "Transform (returns same shape):\n",
      "    A    B\n",
      "0   2   20\n",
      "1   4   40\n",
      "2   6   60\n",
      "3   8   80\n",
      "4  10  100\n",
      "Apply (can return different shape):\n",
      "A     15\n",
      "B    150\n",
      "dtype: int64\n",
      "\n",
      "Using applymap() for element-wise operations:\n",
      "     A    B     C\n",
      "0  0.1  1.0  10.0\n",
      "1  0.2  2.0  20.0\n",
      "2  0.3  3.0  30.0\n",
      "3  0.4  4.0  40.0\n",
      "4  0.5  5.0  50.0\n",
      "\n",
      "Performance comparison (using timeit-like approach):\n",
      ".2f\n",
      ".2f\n",
      "\n",
      "Practical examples:\n",
      "Z-score normalization using transform:\n",
      "          A         B         C\n",
      "0 -1.264911 -1.264911 -1.264911\n",
      "1 -0.632456 -0.632456 -0.632456\n",
      "2  0.000000  0.000000  0.000000\n",
      "3  0.632456  0.632456  0.632456\n",
      "4  1.264911  1.264911  1.264911\n",
      "\n",
      "Percentage change using apply:\n",
      "          A         B         C\n",
      "0       NaN       NaN       NaN\n",
      "1  1.000000  1.000000  1.000000\n",
      "2  0.500000  0.500000  0.500000\n",
      "3  0.333333  0.333333  0.333333\n",
      "4  0.250000  0.250000  0.250000\n",
      "\n",
      "Rolling mean using apply:\n",
      "     A     B      C\n",
      "0  NaN   NaN    NaN\n",
      "1  1.5  15.0  150.0\n",
      "2  2.5  25.0  250.0\n",
      "3  3.5  35.0  350.0\n",
      "4  4.5  45.0  450.0\n",
      "\n",
      "GroupBy with apply:\n",
      "          A   B    C\n",
      "Category            \n",
      "A         9  90  900\n",
      "B         6  60  600\n",
      "\n",
      "GroupBy with transform:\n",
      "     A     B      C\n",
      "0 -2.0 -20.0 -200.0\n",
      "1 -1.0 -10.0 -100.0\n",
      "2  0.0   0.0    0.0\n",
      "3  1.0  10.0  100.0\n",
      "4  2.0  20.0  200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4q/sh_trgw128d534fn5tl4mw5r0000gn/T/ipykernel_44667/1901250216.py:72: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(df[['A', 'B', 'C']].applymap(lambda x: x / 10))\n"
     ]
    }
   ],
   "source": [
    "# Apply, Map, and Transform Examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500],\n",
    "    'Category': ['A', 'B', 'A', 'B', 'A']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Using map() - element-wise transformation on Series\n",
    "print(\"\\nUsing map() to transform Category column:\")\n",
    "category_map = {'A': 'Group Alpha', 'B': 'Group Beta'}\n",
    "print(df['Category'].map(category_map))\n",
    "\n",
    "print(\"\\nUsing map() with function:\")\n",
    "print(df['A'].map(lambda x: x ** 2))\n",
    "\n",
    "# Using apply() on Series\n",
    "print(\"\\nUsing apply() on Series:\")\n",
    "print(df['A'].apply(lambda x: x * 10))\n",
    "\n",
    "print(\"\\nUsing apply() with string operations:\")\n",
    "print(df['Category'].apply(lambda x: x.lower()))\n",
    "\n",
    "# Using apply() on DataFrame - axis=0 (columns)\n",
    "print(\"\\nUsing apply() on DataFrame (axis=0, columns):\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.sum()))\n",
    "\n",
    "# Using apply() on DataFrame - axis=1 (rows)\n",
    "print(\"\\nUsing apply() on DataFrame (axis=1, rows):\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.sum(), axis=1))\n",
    "\n",
    "# Using transform() on DataFrame\n",
    "print(\"\\nUsing transform() on DataFrame:\")\n",
    "print(df[['A', 'B', 'C']].transform(lambda x: x - x.mean()))\n",
    "\n",
    "# More complex examples\n",
    "print(\"\\nComplex apply() example - custom function:\")\n",
    "def categorize_value(x):\n",
    "    if x < 3:\n",
    "        return 'Low'\n",
    "    elif x < 5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "print(df['A'].apply(categorize_value))\n",
    "\n",
    "# Apply with multiple columns\n",
    "print(\"\\nApply with multiple columns:\")\n",
    "def combine_columns(row):\n",
    "    return f\"{row['A']}-{row['B']}-{row['C']}\"\n",
    "\n",
    "print(df.apply(combine_columns, axis=1))\n",
    "\n",
    "# Transform vs Apply comparison\n",
    "print(\"\\nTransform vs Apply comparison:\")\n",
    "print(\"Transform (returns same shape):\")\n",
    "print(df[['A', 'B']].transform(lambda x: x * 2))\n",
    "\n",
    "print(\"Apply (can return different shape):\")\n",
    "print(df[['A', 'B']].apply(lambda x: x.sum()))\n",
    "\n",
    "# Using applymap() for element-wise operations on entire DataFrame\n",
    "print(\"\\nUsing applymap() for element-wise operations:\")\n",
    "print(df[['A', 'B', 'C']].applymap(lambda x: x / 10))\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nPerformance comparison (using timeit-like approach):\")\n",
    "import time\n",
    "\n",
    "# Create larger DataFrame for performance test\n",
    "large_df = pd.DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'])\n",
    "\n",
    "start = time.time()\n",
    "result_apply = large_df.apply(lambda x: x ** 2, axis=0)\n",
    "apply_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result_transform = large_df.transform(lambda x: x ** 2)\n",
    "transform_time = time.time() - start\n",
    "\n",
    "print(\".2f\")\n",
    "print(\".2f\")\n",
    "\n",
    "# Practical examples\n",
    "print(\"\\nPractical examples:\")\n",
    "\n",
    "# Data normalization\n",
    "print(\"Z-score normalization using transform:\")\n",
    "def zscore(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "normalized_df = df[['A', 'B', 'C']].transform(zscore)\n",
    "print(normalized_df)\n",
    "\n",
    "# Percentage change\n",
    "print(\"\\nPercentage change using apply:\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.pct_change()))\n",
    "\n",
    "# Rolling calculations with apply\n",
    "print(\"\\nRolling mean using apply:\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.rolling(2).mean()))\n",
    "\n",
    "# GroupBy with apply/transform\n",
    "print(\"\\nGroupBy with apply:\")\n",
    "grouped = df.groupby('Category')[['A', 'B', 'C']].apply(lambda x: x.sum())\n",
    "print(grouped)\n",
    "\n",
    "print(\"\\nGroupBy with transform:\")\n",
    "grouped_transform = df.groupby('Category')[['A', 'B', 'C']].transform(lambda x: x - x.mean())\n",
    "print(grouped_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83696d",
   "metadata": {},
   "source": [
    "## Memory Optimization Techniques\n",
    "\n",
    "Memory optimization techniques help reduce memory usage and improve performance when working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64921eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply, Map, and Transform Examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500],\n",
    "    'Category': ['A', 'B', 'A', 'B', 'A']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Using map() - element-wise transformation on Series\n",
    "print(\"\\nUsing map() to transform Category column:\")\n",
    "category_map = {'A': 'Group Alpha', 'B': 'Group Beta'}\n",
    "print(df['Category'].map(category_map))\n",
    "\n",
    "print(\"\\nUsing map() with function:\")\n",
    "print(df['A'].map(lambda x: x ** 2))\n",
    "\n",
    "# Using apply() on Series\n",
    "print(\"\\nUsing apply() on Series:\")\n",
    "print(df['A'].apply(lambda x: x * 10))\n",
    "\n",
    "print(\"\\nUsing apply() with string operations:\")\n",
    "print(df['Category'].apply(lambda x: x.lower()))\n",
    "\n",
    "# Using apply() on DataFrame - axis=0 (columns)\n",
    "print(\"\\nUsing apply() on DataFrame (axis=0, columns):\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.sum()))\n",
    "\n",
    "# Using apply() on DataFrame - axis=1 (rows)\n",
    "print(\"\\nUsing apply() on DataFrame (axis=1, rows):\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.sum(), axis=1))\n",
    "\n",
    "# Using transform() on DataFrame\n",
    "print(\"\\nUsing transform() on DataFrame:\")\n",
    "print(df[['A', 'B', 'C']].transform(lambda x: x - x.mean()))\n",
    "\n",
    "# More complex examples\n",
    "print(\"\\nComplex apply() example - custom function:\")\n",
    "def categorize_value(x):\n",
    "    if x < 3:\n",
    "        return 'Low'\n",
    "    elif x < 5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "print(df['A'].apply(categorize_value))\n",
    "\n",
    "# Apply with multiple columns\n",
    "print(\"\\nApply with multiple columns:\")\n",
    "def combine_columns(row):\n",
    "    return f\"{row['A']}-{row['B']}-{row['C']}\"\n",
    "\n",
    "print(df.apply(combine_columns, axis=1))\n",
    "\n",
    "# Transform vs Apply comparison\n",
    "print(\"\\nTransform vs Apply comparison:\")\n",
    "print(\"Transform (returns same shape):\")\n",
    "print(df[['A', 'B']].transform(lambda x: x * 2))\n",
    "\n",
    "print(\"Apply (can return different shape):\")\n",
    "print(df[['A', 'B']].apply(lambda x: x.sum()))\n",
    "\n",
    "# Using applymap() for element-wise operations on entire DataFrame\n",
    "print(\"\\nUsing applymap() for element-wise operations:\")\n",
    "print(df[['A', 'B', 'C']].applymap(lambda x: x / 10))\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nPerformance comparison (using timeit-like approach):\")\n",
    "import time\n",
    "\n",
    "# Create larger DataFrame for performance test\n",
    "large_df = pd.DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'])\n",
    "\n",
    "start = time.time()\n",
    "result_apply = large_df.apply(lambda x: x ** 2, axis=0)\n",
    "apply_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result_transform = large_df.transform(lambda x: x ** 2)\n",
    "transform_time = time.time() - start\n",
    "\n",
    "print(\".2f\")\n",
    "print(\".2f\")\n",
    "\n",
    "# Practical examples\n",
    "print(\"\\nPractical examples:\")\n",
    "\n",
    "# Data normalization\n",
    "print(\"Z-score normalization using transform:\")\n",
    "def zscore(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "normalized_df = df[['A', 'B', 'C']].transform(zscore)\n",
    "print(normalized_df)\n",
    "\n",
    "# Percentage change\n",
    "print(\"\\nPercentage change using apply:\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.pct_change()))\n",
    "\n",
    "# Rolling calculations with apply\n",
    "print(\"\\nRolling mean using apply:\")\n",
    "print(df[['A', 'B', 'C']].apply(lambda x: x.rolling(2).mean()))\n",
    "\n",
    "# GroupBy with apply/transform\n",
    "print(\"\\nGroupBy with apply:\")\n",
    "grouped = df.groupby('Category')[['A', 'B', 'C']].apply(lambda x: x.sum())\n",
    "print(grouped)\n",
    "\n",
    "print(\"\\nGroupBy with transform:\")\n",
    "grouped_transform = df.groupby('Category')[['A', 'B', 'C']].transform(lambda x: x - x.mean())\n",
    "print(grouped_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a66d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Optimization Techniques in Pandas\n",
      "==================================================\n",
      "\n",
      "1. Data Types and Memory Usage\n",
      "Original DataFrame memory usage:\n",
      "Total memory: 8.01 MB\n",
      "Column memory usage:\n",
      "  int_col: 0.76 MB\n",
      "  float_col: 0.76 MB\n",
      "  string_col: 5.63 MB\n",
      "  bool_col: 0.10 MB\n",
      "  date_col: 0.76 MB\n",
      "\n",
      "Optimizing data types:\n",
      "int_col: int64 -> int8\n",
      "float_col: float64 -> float32\n",
      "string_col: object -> category\n",
      "bool_col: bool -> bool\n",
      "\n",
      "Optimized DataFrame memory usage:\n",
      "Total memory: 1.43 MB\n",
      "Column memory usage:\n",
      "  int_col: 0.10 MB\n",
      "  float_col: 0.38 MB\n",
      "  string_col: 0.10 MB\n",
      "  bool_col: 0.10 MB\n",
      "  date_col: 0.76 MB\n",
      ".2f\n",
      "\n",
      "2. Sparse Data Structures\n",
      "Regular DataFrame memory:\n",
      "Total: 1563 KB\n",
      "Sparse DataFrame memory:\n",
      "Total: 1172 KB\n",
      "\n",
      "3. Chunking Large Files\n",
      "Simulating chunked reading (normally used with pd.read_csv):\n",
      "Processed chunk 1, cumulative sum: 3123750\n",
      "Processed chunk 2, cumulative sum: 12497500\n",
      "Processed chunk 3, cumulative sum: 28121250\n",
      "Processed chunk 4, cumulative sum: 49995000\n",
      "\n",
      "4. Memory-Efficient Operations\n",
      "Memory before operations:\n",
      "Total: 391 KB\n",
      "After adding column F:\n",
      "Total: 469 KB\n",
      "After eval() operation:\n",
      "Total: 547 KB\n",
      "\n",
      "5. Categorical Data Benefits\n",
      "String series memory:\n",
      "Total: 4883 KB\n",
      "Categorical series memory:\n",
      "Total: 98 KB\n",
      "Categorical operations:\n",
      "Categories: ['A', 'B', 'C']\n",
      "Codes: [0, 1, 2, 0, 0]\n",
      "\n",
      "6. Memory Monitoring\n",
      ".2f\n",
      ".2f\n",
      ".2f\n",
      "\n",
      "7. Memory Optimization Best Practices:\n",
      "• Use appropriate data types (int8, int16, float32, etc.)\n",
      "• Convert repetitive strings to categorical data\n",
      "• Use sparse data structures for mostly empty data\n",
      "• Process large files in chunks\n",
      "• Use in-place operations when possible\n",
      "• Use eval() and query() for complex operations\n",
      "• Monitor memory usage regularly\n",
      "• Consider using Dask for very large datasets\n",
      "\n",
      "Memory optimization completed!\n"
     ]
    }
   ],
   "source": [
    "# Memory Optimization Techniques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(\"Memory Optimization Techniques in Pandas\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a sample DataFrame with memory inefficiencies\n",
    "print(\"\\n1. Data Types and Memory Usage\")\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'int_col': np.random.randint(0, 100, 100000),  # Will be int64 by default\n",
    "    'float_col': np.random.random(100000),  # Will be float64 by default\n",
    "    'string_col': ['category_' + str(i % 10) for i in range(100000)],  # Repetitive strings\n",
    "    'bool_col': np.random.choice([True, False], 100000),\n",
    "    'date_col': pd.date_range('2020-01-01', periods=100000, freq='1min')\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame memory usage:\")\n",
    "print(f\"Total memory: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(\"Column memory usage:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}: {df[col].memory_usage(deep=True) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Optimize data types\n",
    "print(\"\\nOptimizing data types:\")\n",
    "\n",
    "# Integer downcasting\n",
    "df_optimized = df.copy()\n",
    "df_optimized['int_col'] = pd.to_numeric(df_optimized['int_col'], downcast='integer')\n",
    "print(f\"int_col: {df['int_col'].dtype} -> {df_optimized['int_col'].dtype}\")\n",
    "\n",
    "# Float downcasting\n",
    "df_optimized['float_col'] = pd.to_numeric(df_optimized['float_col'], downcast='float')\n",
    "print(f\"float_col: {df['float_col'].dtype} -> {df_optimized['float_col'].dtype}\")\n",
    "\n",
    "# Convert repetitive strings to category\n",
    "df_optimized['string_col'] = df_optimized['string_col'].astype('category')\n",
    "print(f\"string_col: {df['string_col'].dtype} -> {df_optimized['string_col'].dtype}\")\n",
    "\n",
    "# Boolean optimization (already bool, but showing the concept)\n",
    "df_optimized['bool_col'] = df_optimized['bool_col'].astype('bool')\n",
    "print(f\"bool_col: {df['bool_col'].dtype} -> {df_optimized['bool_col'].dtype}\")\n",
    "\n",
    "print(\"\\nOptimized DataFrame memory usage:\")\n",
    "print(f\"Total memory: {df_optimized.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(\"Column memory usage:\")\n",
    "for col in df_optimized.columns:\n",
    "    print(f\"  {col}: {df_optimized[col].memory_usage(deep=True) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "memory_saved = (df.memory_usage(deep=True).sum() - df_optimized.memory_usage(deep=True).sum()) / 1024 / 1024\n",
    "print(\".2f\")\n",
    "\n",
    "# 2. Sparse data structures\n",
    "print(\"\\n2. Sparse Data Structures\")\n",
    "sparse_data = pd.DataFrame({\n",
    "    'mostly_zeros': [0] * 100000 + [1] * 10,\n",
    "    'mostly_null': [None] * 99990 + list(range(10)) + [None] * 10  # Fixed length\n",
    "})\n",
    "print(\"Regular DataFrame memory:\")\n",
    "print(f\"Total: {sparse_data.memory_usage(deep=True).sum() / 1024:.0f} KB\")\n",
    "\n",
    "# Convert to sparse\n",
    "sparse_data_sparse = sparse_data.astype(pd.SparseDtype(\"float64\", np.nan))\n",
    "print(\"Sparse DataFrame memory:\")\n",
    "print(f\"Total: {sparse_data_sparse.memory_usage(deep=True).sum() / 1024:.0f} KB\")\n",
    "\n",
    "# 3. Chunking for large files\n",
    "print(\"\\n3. Chunking Large Files\")\n",
    "print(\"Simulating chunked reading (normally used with pd.read_csv):\")\n",
    "\n",
    "# Create a large CSV-like data\n",
    "large_data = pd.DataFrame({\n",
    "    'A': range(10000),\n",
    "    'B': np.random.randn(10000),\n",
    "    'C': ['cat'] * 5000 + ['dog'] * 5000\n",
    "})\n",
    "\n",
    "# Simulate chunked processing\n",
    "chunk_size = 2500\n",
    "total_sum = 0\n",
    "for i in range(0, len(large_data), chunk_size):\n",
    "    chunk = large_data.iloc[i:i+chunk_size]\n",
    "    total_sum += chunk['A'].sum()\n",
    "    print(f\"Processed chunk {i//chunk_size + 1}, cumulative sum: {total_sum}\")\n",
    "\n",
    "# 4. Memory-efficient operations\n",
    "print(\"\\n4. Memory-Efficient Operations\")\n",
    "\n",
    "# Instead of creating copies\n",
    "df_large = pd.DataFrame(np.random.randn(10000, 5), columns=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "print(\"Memory before operations:\")\n",
    "print(f\"Total: {df_large.memory_usage(deep=True).sum() / 1024:.0f} KB\")\n",
    "\n",
    "# In-place operations\n",
    "df_large['F'] = df_large['A'] + df_large['B']  # Creates new column\n",
    "print(\"After adding column F:\")\n",
    "print(f\"Total: {df_large.memory_usage(deep=True).sum() / 1024:.0f} KB\")\n",
    "\n",
    "# Using eval() for memory-efficient operations\n",
    "df_eval = df_large.copy()\n",
    "df_eval = df_eval.eval('G = A * B + C')\n",
    "print(\"After eval() operation:\")\n",
    "print(f\"Total: {df_eval.memory_usage(deep=True).sum() / 1024:.0f} KB\")\n",
    "\n",
    "# 5. Categorical data benefits\n",
    "print(\"\\n5. Categorical Data Benefits\")\n",
    "text_data = pd.Series(['A', 'B', 'C', 'A'] * 25000)\n",
    "print(\"String series memory:\")\n",
    "print(f\"Total: {text_data.memory_usage(deep=True) / 1024:.0f} KB\")\n",
    "\n",
    "categorical_data = text_data.astype('category')\n",
    "print(\"Categorical series memory:\")\n",
    "print(f\"Total: {categorical_data.memory_usage(deep=True) / 1024:.0f} KB\")\n",
    "\n",
    "# Operations on categorical data\n",
    "print(\"Categorical operations:\")\n",
    "print(\"Categories:\", categorical_data.cat.categories.tolist())\n",
    "print(\"Codes:\", categorical_data.cat.codes.head().tolist())\n",
    "\n",
    "# 6. Memory monitoring\n",
    "print(\"\\n6. Memory Monitoring\")\n",
    "def get_memory_usage(df):\n",
    "    return df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "\n",
    "df_test = pd.DataFrame(np.random.randn(50000, 3), columns=['X', 'Y', 'Z'])\n",
    "print(\".2f\")\n",
    "\n",
    "# Add some operations and monitor\n",
    "df_test['W'] = df_test['X'] ** 2\n",
    "print(\".2f\")\n",
    "\n",
    "df_test = df_test[df_test['W'] > 0]  # Filtering\n",
    "print(\".2f\")\n",
    "\n",
    "# 7. Best practices summary\n",
    "print(\"\\n7. Memory Optimization Best Practices:\")\n",
    "print(\"• Use appropriate data types (int8, int16, float32, etc.)\")\n",
    "print(\"• Convert repetitive strings to categorical data\")\n",
    "print(\"• Use sparse data structures for mostly empty data\")\n",
    "print(\"• Process large files in chunks\")\n",
    "print(\"• Use in-place operations when possible\")\n",
    "print(\"• Use eval() and query() for complex operations\")\n",
    "print(\"• Monitor memory usage regularly\")\n",
    "print(\"• Consider using Dask for very large datasets\")\n",
    "\n",
    "print(\"\\nMemory optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18bda928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Large Datasets in Pandas\n",
      "========================================\n",
      "\n",
      "1. Chunked Reading with read_csv\n",
      "Creating sample large CSV file...\n",
      "Created large_dataset.csv with 100000 rows\n",
      "\n",
      "Reading in chunks of 10000 rows:\n",
      "Processed chunk with 10000 rows, total so far: 10000\n",
      "Processed chunk with 10000 rows, total so far: 20000\n",
      "Processed chunk with 10000 rows, total so far: 30000\n",
      "Processed chunk with 10000 rows, total so far: 40000\n",
      "Processed chunk with 10000 rows, total so far: 50000\n",
      "Processed chunk with 10000 rows, total so far: 60000\n",
      "Processed chunk with 10000 rows, total so far: 70000\n",
      "Processed chunk with 10000 rows, total so far: 80000\n",
      "Processed chunk with 10000 rows, total so far: 90000\n",
      "Processed chunk with 10000 rows, total so far: 100000\n",
      ".2f\n",
      "Category distribution: {'A': 24954, 'D': 24885, 'B': 25158, 'C': 25003}\n",
      "\n",
      "2. Memory-Efficient Data Types During Reading\n",
      "Reading with optimized dtypes:\n",
      "Memory usage with optimized dtypes: 1.62 MB\n",
      "Memory usage with default dtypes: 7.06 MB\n",
      "\n",
      "3. Selective Column Reading\n",
      "Reading only necessary columns:\n",
      "Memory usage with subset of columns: 1.24 MB\n",
      "Shape: (100000, 3)\n",
      "\n",
      "4. Filtering During Reading\n",
      "Using skiprows to filter data:\n",
      "Filtered dataset shape: (9914, 4)\n",
      "Memory usage: 0.16 MB\n",
      "\n",
      "5. Introduction to Dask for Large Datasets\n",
      "Dask not available. Install with: pip install dask\n",
      "\n",
      "6. Efficient GroupBy Operations on Large Data\n",
      "Using categorical data for faster groupby:\n",
      ".2f\n",
      "              mean       std  count\n",
      "category                           \n",
      "A         0.004024  0.999902  24954\n",
      "B        -0.012093  0.995738  25158\n",
      "C         0.006051  1.004232  25003\n",
      "D        -0.007676  0.999689  24885\n",
      "\n",
      "7. Out-of-Memory Operations\n",
      "Simulating operations that would normally cause memory issues:\n",
      "Processing in batches:\n",
      "Batch processing result:\n",
      "category\n",
      "A    0.003983\n",
      "B   -0.012107\n",
      "C    0.006020\n",
      "D   -0.007732\n",
      "Name: value, dtype: float32\n",
      "\n",
      "8. Memory Monitoring and Cleanup\n",
      "Memory usage before cleanup:\n",
      ".2f\n",
      "Memory usage after cleanup:\n",
      ".2f\n",
      "\n",
      "9. Best Practices for Large Datasets:\n",
      "• Use chunked reading with pd.read_csv(chunksize=...)\n",
      "• Specify dtypes during reading to reduce memory usage\n",
      "• Read only necessary columns with usecols\n",
      "• Use categorical data for repetitive strings\n",
      "• Filter data early in the pipeline\n",
      "• Use Dask for datasets larger than RAM\n",
      "• Process data in batches for memory-intensive operations\n",
      "• Monitor memory usage and clean up unused objects\n",
      "• Consider using databases for very large datasets\n",
      "• Use feather or parquet formats for intermediate storage\n",
      "\n",
      "Large dataset handling completed!\n",
      "Cleaned up large_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4q/sh_trgw128d534fn5tl4mw5r0000gn/T/ipykernel_44667/2083238726.py:116: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grouped_result = df_optimized.groupby('category')['value'].agg(['mean', 'std', 'count'])\n",
      "/var/folders/4q/sh_trgw128d534fn5tl4mw5r0000gn/T/ipykernel_44667/2083238726.py:134: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  batch_result = batch.groupby('category')['value'].mean()\n",
      "/var/folders/4q/sh_trgw128d534fn5tl4mw5r0000gn/T/ipykernel_44667/2083238726.py:138: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  final_result = pd.concat(results).groupby(level=0).mean()\n"
     ]
    }
   ],
   "source": [
    "# Handling Large Datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Handling Large Datasets in Pandas\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Chunked reading with read_csv\n",
    "print(\"\\n1. Chunked Reading with read_csv\")\n",
    "print(\"Creating sample large CSV file...\")\n",
    "\n",
    "# Create a large CSV file for demonstration\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(100000),\n",
    "    'value': np.random.randn(100000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 100000),\n",
    "    'timestamp': pd.date_range('2020-01-01', periods=100000, freq='1min')\n",
    "})\n",
    "\n",
    "# Save to CSV (normally this would be a large existing file)\n",
    "csv_file = 'large_dataset.csv'\n",
    "large_df.to_csv(csv_file, index=False)\n",
    "print(f\"Created {csv_file} with {len(large_df)} rows\")\n",
    "\n",
    "# Read in chunks\n",
    "print(\"\\nReading in chunks of 10000 rows:\")\n",
    "chunk_size = 10000\n",
    "total_rows = 0\n",
    "category_counts = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    total_rows += len(chunk)\n",
    "    # Process each chunk\n",
    "    chunk_category_counts = chunk['category'].value_counts()\n",
    "    for cat, count in chunk_category_counts.items():\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + count\n",
    "    print(f\"Processed chunk with {len(chunk)} rows, total so far: {total_rows}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\".2f\")\n",
    "print(\"Category distribution:\", category_counts)\n",
    "\n",
    "# 2. Memory-efficient data types\n",
    "print(\"\\n2. Memory-Efficient Data Types During Reading\")\n",
    "print(\"Reading with optimized dtypes:\")\n",
    "\n",
    "# Define dtypes for memory efficiency\n",
    "dtypes = {\n",
    "    'id': 'int32',  # Down from int64\n",
    "    'value': 'float32',  # Down from float64\n",
    "    'category': 'category',  # Convert to categorical\n",
    "    # Note: timestamp handled separately with parse_dates\n",
    "}\n",
    "\n",
    "df_optimized = pd.read_csv(csv_file, dtype=dtypes, parse_dates=['timestamp'])\n",
    "print(f\"Memory usage with optimized dtypes: {df_optimized.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Compare with default reading\n",
    "df_default = pd.read_csv(csv_file, parse_dates=['timestamp'])\n",
    "print(f\"Memory usage with default dtypes: {df_default.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 3. Selective column reading\n",
    "print(\"\\n3. Selective Column Reading\")\n",
    "print(\"Reading only necessary columns:\")\n",
    "\n",
    "# Read only specific columns\n",
    "columns_to_read = ['id', 'category', 'timestamp']\n",
    "df_subset = pd.read_csv(csv_file, usecols=columns_to_read, dtype={'id': 'int32', 'category': 'category'}, parse_dates=['timestamp'])\n",
    "print(f\"Memory usage with subset of columns: {df_subset.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Shape: {df_subset.shape}\")\n",
    "\n",
    "# 4. Filtering during reading\n",
    "print(\"\\n4. Filtering During Reading\")\n",
    "print(\"Using skiprows to filter data:\")\n",
    "\n",
    "# Skip rows based on condition (simulate filtering)\n",
    "def row_filter(index):\n",
    "    if index == 0:  # Keep header\n",
    "        return False\n",
    "    # Skip 90% of rows randomly for demonstration\n",
    "    return np.random.random() > 0.1\n",
    "\n",
    "df_filtered = pd.read_csv(csv_file, skiprows=lambda x: row_filter(x), dtype=dtypes, parse_dates=['timestamp'])\n",
    "print(f\"Filtered dataset shape: {df_filtered.shape}\")\n",
    "print(f\"Memory usage: {df_filtered.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 5. Processing large datasets with Dask (if available)\n",
    "print(\"\\n5. Introduction to Dask for Large Datasets\")\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    print(\"Dask is available - demonstrating lazy evaluation:\")\n",
    "\n",
    "    # Create Dask DataFrame\n",
    "    ddf = dd.from_pandas(large_df, npartitions=4)\n",
    "    print(f\"Dask DataFrame with {ddf.npartitions} partitions\")\n",
    "    print(f\"Shape: {ddf.compute().shape}\")  # .compute() triggers actual computation\n",
    "\n",
    "    # Lazy operations\n",
    "    result = ddf[ddf['value'] > 0]['category'].value_counts()\n",
    "    print(\"Lazy operation defined, computing result:\")\n",
    "    print(result.compute())\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Dask not available. Install with: pip install dask\")\n",
    "\n",
    "# 6. Efficient groupby operations\n",
    "print(\"\\n6. Efficient GroupBy Operations on Large Data\")\n",
    "print(\"Using categorical data for faster groupby:\")\n",
    "\n",
    "# Ensure category is categorical\n",
    "df_optimized['category'] = df_optimized['category'].astype('category')\n",
    "\n",
    "start_time = time.time()\n",
    "grouped_result = df_optimized.groupby('category')['value'].agg(['mean', 'std', 'count'])\n",
    "groupby_time = time.time() - start_time\n",
    "print(\".2f\")\n",
    "print(grouped_result)\n",
    "\n",
    "# 7. Out-of-memory operations\n",
    "print(\"\\n7. Out-of-Memory Operations\")\n",
    "print(\"Simulating operations that would normally cause memory issues:\")\n",
    "\n",
    "# Instead of loading everything at once\n",
    "print(\"Processing in batches:\")\n",
    "\n",
    "batch_size = 25000\n",
    "results = []\n",
    "\n",
    "for i in range(0, len(df_optimized), batch_size):\n",
    "    batch = df_optimized.iloc[i:i+batch_size]\n",
    "    # Perform operation on batch\n",
    "    batch_result = batch.groupby('category')['value'].mean()\n",
    "    results.append(batch_result)\n",
    "\n",
    "# Combine results\n",
    "final_result = pd.concat(results).groupby(level=0).mean()\n",
    "print(\"Batch processing result:\")\n",
    "print(final_result)\n",
    "\n",
    "# 8. Monitoring and cleanup\n",
    "print(\"\\n8. Memory Monitoring and Cleanup\")\n",
    "import gc\n",
    "\n",
    "print(\"Memory usage before cleanup:\")\n",
    "print(\".2f\")\n",
    "\n",
    "# Delete large DataFrames\n",
    "del large_df, df_default, df_subset, df_filtered\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory usage after cleanup:\")\n",
    "print(\".2f\")\n",
    "\n",
    "# 9. Best practices for large datasets\n",
    "print(\"\\n9. Best Practices for Large Datasets:\")\n",
    "print(\"• Use chunked reading with pd.read_csv(chunksize=...)\")\n",
    "print(\"• Specify dtypes during reading to reduce memory usage\")\n",
    "print(\"• Read only necessary columns with usecols\")\n",
    "print(\"• Use categorical data for repetitive strings\")\n",
    "print(\"• Filter data early in the pipeline\")\n",
    "print(\"• Use Dask for datasets larger than RAM\")\n",
    "print(\"• Process data in batches for memory-intensive operations\")\n",
    "print(\"• Monitor memory usage and clean up unused objects\")\n",
    "print(\"• Consider using databases for very large datasets\")\n",
    "print(\"• Use feather or parquet formats for intermediate storage\")\n",
    "\n",
    "print(\"\\nLarge dataset handling completed!\")\n",
    "\n",
    "# Clean up the CSV file\n",
    "import os\n",
    "os.remove(csv_file)\n",
    "print(f\"Cleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffdd58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration with Other Libraries\n",
      "===================================\n",
      "\n",
      "1. Scikit-learn Integration\n",
      "Preparing data for machine learning:\n",
      "Sample DataFrame:\n",
      "   feature1  feature2  feature3 category  target\n",
      "0  0.496714  2.798711         2        A       1\n",
      "1 -0.138264  1.849267         2        C       1\n",
      "2  0.647689  0.119261         2        A       1\n",
      "3  1.523030 -1.293874         3        C       1\n",
      "4 -0.234153  1.396447         4        A       0\n",
      "\n",
      "Shape: (1000, 5)\n",
      "\n",
      "Data preprocessing:\n",
      "After one-hot encoding:\n",
      "   feature1  feature2  feature3  target  category_B  category_C\n",
      "0  0.496714  2.798711         2       1       False       False\n",
      "1 -0.138264  1.849267         2       1       False        True\n",
      "2  0.647689  0.119261         2       1       False       False\n",
      "3  1.523030 -1.293874         3       1       False        True\n",
      "4 -0.234153  1.396447         4       0       False       False\n",
      "\n",
      "Features shape: (1000, 5)\n",
      "Target shape: (1000,)\n",
      "\n",
      "After standardization:\n",
      "   feature1  feature2  feature3  category_B  category_C\n",
      "0  0.487759  1.332576  0.031453   -0.703398   -0.740262\n",
      "1 -0.161022  0.856405  0.031453   -0.703398    1.350873\n",
      "2  0.642015 -0.011240  0.031453   -0.703398   -0.740262\n",
      "3  1.536382 -0.719965  0.730399   -0.703398    1.350873\n",
      "4 -0.258995  0.629303  1.429345   -0.703398   -0.740262\n",
      "\n",
      "2. Model Training and Evaluation\n",
      "Train shape: (800, 5), Test shape: (200, 5)\n",
      ".2f\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.57      0.55        99\n",
      "           1       0.55      0.51      0.53       101\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.54      0.54      0.54       200\n",
      "weighted avg       0.54      0.54      0.54       200\n",
      "\n",
      "\n",
      "3. Cross-Validation with Pandas\n",
      "Cross-validation scores:\n",
      ".2f\n",
      ".2f\n",
      ".2f\n",
      ".2f\n",
      ".2f\n",
      ".2f\n",
      "\n",
      "4. Feature Importance Analysis\n",
      "Feature importance:\n",
      "      feature  importance\n",
      "3  category_B    0.131846\n",
      "1    feature2    0.074452\n",
      "4  category_C    0.065972\n",
      "0    feature1    0.019166\n",
      "2    feature3    0.008414\n",
      "\n",
      "5. Scikit-learn Pipeline with Pandas\n",
      ".2f\n",
      "\n",
      "6. Time Series Analysis\n",
      "Time series data:\n",
      "                 value  feature1  feature2\n",
      "date                                      \n",
      "2020-01-01  100.064167 -2.588417  0.014508\n",
      "2020-01-02   99.704061  0.059768 -0.048565\n",
      "2020-01-03   98.812843  0.949523 -0.048007\n",
      "2020-01-04   98.751003 -1.013806  0.119946\n",
      "2020-01-05   99.603598  0.847932  0.054897\n",
      ".2f\n",
      "\n",
      "7. Integration with Other Libraries\n",
      "NumPy integration:\n",
      "Pandas DataFrame to NumPy array: (1000, 4)\n",
      "Correlation plot saved as 'correlation_plot.png'\n",
      "Statsmodels not available\n",
      "\n",
      "8. Best Practices for ML Integration:\n",
      "• Use pandas for data exploration and preprocessing\n",
      "• Convert categorical variables appropriately (one-hot, label encoding)\n",
      "• Handle missing values before ML\n",
      "• Scale/normalize features when necessary\n",
      "• Use pipelines for reproducible preprocessing\n",
      "• Validate models with cross-validation\n",
      "• Analyze feature importance\n",
      "• Consider time series structure for temporal data\n",
      "• Integrate with visualization libraries for insights\n",
      "\n",
      "Library integration completed!\n",
      "Cleaned up correlation plot\n"
     ]
    }
   ],
   "source": [
    "# Integration with Other Libraries (Scikit-learn for ML)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Integration with Other Libraries\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Scikit-learn Integration\n",
    "print(\"\\n1. Scikit-learn Integration\")\n",
    "print(\"Preparing data for machine learning:\")\n",
    "\n",
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'feature1': np.random.randn(1000),\n",
    "    'feature2': np.random.randn(1000) * 2,\n",
    "    'feature3': np.random.randint(0, 5, 1000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "    'target': np.random.choice([0, 1], 1000)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "\n",
    "# Data preprocessing with pandas\n",
    "print(\"\\nData preprocessing:\")\n",
    "\n",
    "# Handle categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=['category'], drop_first=True)\n",
    "print(\"After one-hot encoding:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Feature scaling preparation\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop('target', axis=1)\n",
    "y = df_encoded['target']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(X_scaled.head())\n",
    "\n",
    "# 2. Model training and evaluation\n",
    "print(\"\\n2. Model Training and Evaluation\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\".2f\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 3. Cross-validation with pandas\n",
    "print(\"\\n3. Cross-Validation with Pandas\")\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(\".2f\")\n",
    "print(\".2f\")\n",
    "\n",
    "# 4. Feature importance analysis\n",
    "print(\"\\n4. Feature Importance Analysis\")\n",
    "# Get feature coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': np.abs(model.coef_[0])\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# 5. Pipeline integration\n",
    "print(\"\\n5. Scikit-learn Pipeline with Pandas\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create a pipeline that works directly with pandas\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = ['feature1', 'feature2', 'feature3']\n",
    "categorical_features = ['category']\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Prepare data for pipeline (original df without encoding)\n",
    "X_pipeline = df.drop('target', axis=1)\n",
    "y_pipeline = df['target']\n",
    "\n",
    "# Split and train\n",
    "X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(\n",
    "    X_pipeline, y_pipeline, test_size=0.2, random_state=42, stratify=y_pipeline\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train_pipe, y_train_pipe)\n",
    "pipeline_score = pipeline.score(X_test_pipe, y_test_pipe)\n",
    "print(\".2f\")\n",
    "\n",
    "# 6. Time series with pandas and sklearn\n",
    "print(\"\\n6. Time Series Analysis\")\n",
    "# Create time series data\n",
    "dates = pd.date_range('2020-01-01', periods=365, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': np.random.randn(365).cumsum() + 100,\n",
    "    'feature1': np.random.randn(365),\n",
    "    'feature2': np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.randn(365) * 0.1\n",
    "})\n",
    "\n",
    "ts_data.set_index('date', inplace=True)\n",
    "print(\"Time series data:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# Create lag features\n",
    "ts_data['value_lag1'] = ts_data['value'].shift(1)\n",
    "ts_data['value_lag7'] = ts_data['value'].shift(7)\n",
    "ts_data.dropna(inplace=True)\n",
    "\n",
    "# Prepare for modeling\n",
    "X_ts = ts_data[['feature1', 'feature2', 'value_lag1', 'value_lag7']]\n",
    "y_ts = ts_data['value']\n",
    "\n",
    "# Train simple model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(\n",
    "    X_ts, y_ts, test_size=0.2, shuffle=False  # Don't shuffle time series\n",
    ")\n",
    "\n",
    "ts_model = LinearRegression()\n",
    "ts_model.fit(X_train_ts, y_train_ts)\n",
    "y_pred_ts = ts_model.predict(X_test_ts)\n",
    "\n",
    "mse = mean_squared_error(y_test_ts, y_pred_ts)\n",
    "print(\".2f\")\n",
    "\n",
    "# 7. Integration with other libraries\n",
    "print(\"\\n7. Integration with Other Libraries\")\n",
    "\n",
    "# With NumPy\n",
    "print(\"NumPy integration:\")\n",
    "numpy_array = df.select_dtypes(include=[np.number]).values\n",
    "print(f\"Pandas DataFrame to NumPy array: {numpy_array.shape}\")\n",
    "\n",
    "# With Matplotlib/Seaborn\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Create correlation plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_plot.png', dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Correlation plot saved as 'correlation_plot.png'\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Matplotlib/Seaborn not available for plotting\")\n",
    "\n",
    "# With Statsmodels\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Add constant for intercept\n",
    "    X_sm = sm.add_constant(X_scaled)\n",
    "    sm_model = sm.OLS(y, X_sm).fit()\n",
    "\n",
    "    print(\"Statsmodels OLS summary:\")\n",
    "    print(sm_model.summary().tables[1])  # Coefficients table\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Statsmodels not available\")\n",
    "\n",
    "# 8. Best practices for ML integration\n",
    "print(\"\\n8. Best Practices for ML Integration:\")\n",
    "print(\"• Use pandas for data exploration and preprocessing\")\n",
    "print(\"• Convert categorical variables appropriately (one-hot, label encoding)\")\n",
    "print(\"• Handle missing values before ML\")\n",
    "print(\"• Scale/normalize features when necessary\")\n",
    "print(\"• Use pipelines for reproducible preprocessing\")\n",
    "print(\"• Validate models with cross-validation\")\n",
    "print(\"• Analyze feature importance\")\n",
    "print(\"• Consider time series structure for temporal data\")\n",
    "print(\"• Integrate with visualization libraries for insights\")\n",
    "\n",
    "print(\"\\nLibrary integration completed!\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "if os.path.exists('correlation_plot.png'):\n",
    "    os.remove('correlation_plot.png')\n",
    "    print(\"Cleaned up correlation plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
